{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29c9145",
   "metadata": {},
   "source": [
    "### Import der wichtigsten Bibliotheken für Modelltraining und Monitoring \n",
    "In diesem Abschnitt werden PyTorch-Module für neuronale Netzwerke und Datenverarbeitung, Hugging Face-Tools für Tokenisierung und das Laden von Datensätzen sowie Weights & Biases (wandb) für das Tracking und die Visualisierung von Experimenten importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84f674",
   "metadata": {},
   "source": [
    "### Laden des Wikitext-2-Datensatzes\n",
    "Hier wird der Wikitext-2-raw-v1-Datensatz mit Hugging Face's load_dataset-Funktion geladen, um ihn für das Training oder die Evaluierung eines Modells vorzubereiten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2149e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc823a",
   "metadata": {},
   "source": [
    "### Initialisierung und Anpassung des GPT-2-Tokenizers \n",
    "Der vortrainierte GPT-2-Tokenizer wird geladen und das Padding-Token wird auf das End-of-Sequence-Token (eos_token) gesetzt, um die Konsistenz bei der Textverarbeitung sicherzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure EOS is used as pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77deef7",
   "metadata": {},
   "source": [
    "### Tokenisierung und Vorbereitung der Trainings- und Validierungsdaten\n",
    "Eine Tokenisierungsfunktion wird definiert, die Texte auf eine maximale Länge von 256 Tokens zuschneidet und auffüllt. Anschließend werden der Trainings- und Validierungsdatensatz mithilfe dieser Funktion verarbeitet und die ursprüngliche Textspalte entfernt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0a1c4",
   "metadata": {},
   "source": [
    "### Formatierung der Datensätze für PyTorch\n",
    "Die Trainings- und Validierungsdatensätze werden so formatiert, dass nur die input_ids im PyTorch-Format ausgegeben werden – ideal für das direkte Training mit Modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fab4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe95df6",
   "metadata": {},
   "source": [
    "### Definition der Positional Encoding Klasse\n",
    "Diese Klasse implementiert Positional Encoding, um Positionsinformationen durch Sinus- und Kosinusfunktionen zu den Eingabe-Embeddings hinzuzufügen – ein essenzieller Bestandteil bei der Verarbeitung von Sequenzdaten in Transformermodellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d9c67",
   "metadata": {},
   "source": [
    "### Implementierung eines einfachen Transformer-Decodermodells\n",
    "Diese Klasse definiert ein Transformer-basiertes Decoder-Modell mit Embedding-, Positionskodierungs- und Decoder-Schichten. Sie verarbeitet Eingabesequenzen autoregressiv und projiziert die Ausgaben zurück auf den Wortschatzraum für Aufgaben wie Sprachmodellierung.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_seq_len)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n",
    "    def forward(self, tgt_ids):\n",
    "        \"\"\"\n",
    "        tgt_ids: [batch_size, seq_len] - token ids\n",
    "        \"\"\"\n",
    "        device = tgt_ids.device\n",
    "        x = self.embedding(tgt_ids) * (self.d_model ** 0.5)  # scale embeddings\n",
    "        x = self.pos_encoding(x).transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = x.size(0)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "        # Fake memory — just pass zeros to satisfy TransformerDecoder API\n",
    "        memory = torch.zeros_like(x)\n",
    "\n",
    "        output = self.transformer_decoder(tgt=x, memory=memory, tgt_mask=tgt_mask)\n",
    "        output = output.transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        return self.output_layer(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60504",
   "metadata": {},
   "source": [
    "### Trainingsschleife für das Transformer-Decodermodell\n",
    "Diese Funktion führt das Training des Modells durch, indem sie Eingabesequenzen vorbereitet, Vorhersagen erzeugt, den Verlust berechnet, Gradienten zurückpropagiert und die Modellparameter optimiert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)  # Pass only inputs\n",
    "        loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b559f31",
   "metadata": {},
   "source": [
    "### Evaluierungsfunktion für das Transformer-Decodermodell\n",
    "Diese Funktion bewertet das Modell auf Validierungsdaten, indem sie den Verlust ohne Gradientenberechnung ermittelt, um die Trainingsqualität objektiv zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            inputs = input_ids[:, :-1]\n",
    "            targets = input_ids[:, 1:]\n",
    "\n",
    "            output = model(inputs)  # Only inputs\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389c274",
   "metadata": {},
   "source": [
    "### Initialisierung von Weights & Biases für Experiment-Tracking\n",
    "Mit wandb.login() wird die Verbindung zu Weights & Biases hergestellt. Danach wird die Konfiguration für das Experiment festgelegt, einschließlich Hyperparametern und Modellarchitektur. wandb.init() startet das Tracking des Trainingsprozesses unter dem Projekt \"language-model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af551924",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()  # Only required once per session/machine\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"architecture\": \"TransformerDecoder\",\n",
    "    \"dataset\": \"WikiText-2\",\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"embedding_dim\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"max_seq_len\": 256\n",
    "}\n",
    "\n",
    "wandb.init(project=\"language-model\", config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f0083",
   "metadata": {},
   "source": [
    "### Modellinitialisierung und Setup von Optimizer und Dataloader\n",
    "Ein SimpleTransformerDecoderModel wird mit den aus Weights & Biases (wandb.config) entnommenen Hyperparametern erstellt. Der Adam-Optimizer und die Kreuzentropie-Verlustfunktion werden festgelegt. Zudem werden Dataloader für Training und Validierung vorbereitet, um die Daten in Batches zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92267849",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformerDecoderModel(\n",
    "    vocab_size=wandb.config.vocab_size,\n",
    "    d_model=wandb.config.embedding_dim,\n",
    "    nhead=wandb.config.nhead,\n",
    "    num_layers=wandb.config.num_layers,\n",
    "    max_seq_len=wandb.config.max_seq_len\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9466fda",
   "metadata": {},
   "source": [
    "### Training und Evaluierung über Epochen mit Weights & Biases Logging\n",
    "Für jede Epoche wird das Modell trainiert und evaluiert. Der Trainings- und Validierungsverlust wird berechnet und ausgedruckt. Diese Werte werden dann zu Weights & Biases geloggt, um den Fortschritt des Experiments zu überwachen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e512745",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaca17",
   "metadata": {},
   "source": [
    "### Speichern des Modells und Tokenizers\n",
    "Das Modell wird in einem angegebenen Verzeichnis gespeichert, indem die Gewichtungen mit torch.save gesichert werden. Zudem wird der Tokenizer im gleichen Verzeichnis mit save_pretrained abgelegt, um das Modell später wieder zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "model_save_path = \"./simple_transformer_model\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), model_save_path + \"/pytorch_model.bin\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb33eba",
   "metadata": {},
   "source": [
    "### Laden des gespeicherten Modells und Setzen auf Evaluierungsmodus\n",
    "Das Modell wird mit der gleichen Architektur neu erstellt und die gespeicherten Gewichtungen werden mit load_state_dict geladen. Anschließend wird das Modell in den Evaluierungsmodus versetzt, um Vorhersagen zu treffen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the model class\n",
    "model = SimpleTransformerDecoderModel(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    max_seq_len=256\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(model_save_path + \"/pytorch_model.bin\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a598b",
   "metadata": {},
   "source": [
    "### Textgenerierung mit dem Transformer-Modell\n",
    "Diese Funktion verwendet das Modell, um Text basierend auf einem Eingabe-Prompt zu generieren. Sie tokenisiert den Prompt, verwendet das Modell zur Vorhersage der nächsten Tokens und dekodiert schließlich die generierten Tokens zurück in lesbaren Text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    model.eval()\n",
    "    # Tokenize the prompt text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text (we only use `tgt` part, no memory required for decoding)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_ids)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # Sample from the distribution of possible next tokens\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c7bcd",
   "metadata": {},
   "source": [
    "### Authentifizierung bei Hugging Face Hub\n",
    "Mit der login()-Funktion von Hugging Face wird der Benutzer mit einem API-Token authentifiziert, um auf Modelle und Datasets vom Hugging Face Hub zuzugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbec0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fe67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
