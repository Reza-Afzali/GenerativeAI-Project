{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d153247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2149e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c6d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure EOS is used as pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec14f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fab4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fa82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c728323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_seq_len)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n",
    "    def forward(self, tgt_ids):\n",
    "        \"\"\"\n",
    "        tgt_ids: [batch_size, seq_len] - token ids\n",
    "        \"\"\"\n",
    "        device = tgt_ids.device\n",
    "        x = self.embedding(tgt_ids) * (self.d_model ** 0.5)  # scale embeddings\n",
    "        x = self.pos_encoding(x).transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = x.size(0)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "        # Fake memory â€” just pass zeros to satisfy TransformerDecoder API\n",
    "        memory = torch.zeros_like(x)\n",
    "\n",
    "        output = self.transformer_decoder(tgt=x, memory=memory, tgt_mask=tgt_mask)\n",
    "        output = output.transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        return self.output_layer(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9912db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)  # Pass only inputs\n",
    "        loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aabdac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            inputs = input_ids[:, :-1]\n",
    "            targets = input_ids[:, 1:]\n",
    "\n",
    "            output = model(inputs)  # Only inputs\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af551924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-valley-8</strong> at: <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/s3fa9zet' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/s3fa9zet</a><br> View project at: <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250426_004434-s3fa9zet\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie1\\Desktop\\test project\\wandb\\run-20250426_004546-a58low11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/a58low11' target=\"_blank\">major-snowflake-9</a></strong> to <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/a58low11' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/a58low11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/a58low11?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2408aceb450>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()  # Only required once per session/machine\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"architecture\": \"TransformerDecoder\",\n",
    "    \"dataset\": \"WikiText-2\",\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"embedding_dim\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"max_seq_len\": 256\n",
    "}\n",
    "\n",
    "wandb.init(project=\"language-model\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92267849",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformerDecoderModel(\n",
    "    vocab_size=wandb.config.vocab_size,\n",
    "    d_model=wandb.config.embedding_dim,\n",
    "    nhead=wandb.config.nhead,\n",
    "    num_layers=wandb.config.num_layers,\n",
    "    max_seq_len=wandb.config.max_seq_len\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e512745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 1.5488 | Val Loss: 1.5049\n",
      "Epoch 2\n",
      "Train Loss: 1.3501 | Val Loss: 1.4618\n",
      "Epoch 3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0607c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
